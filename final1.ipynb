{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j86EM4fcJYtJ"
      },
      "outputs": [],
      "source": [
        "#t1\n",
        "sample1 = data1.sample(105, random_state=56)\n",
        "sample1.isna().sum()\n",
        "sample1['total_bedrooms'] = sample1['total_bedrooms'].fillna(sample1['total_bedrooms']).mean()\n",
        "sample1.isna().sum()\n",
        "sample1.head()\n",
        "sample1_ohe = pd.get_dummies(sample1, columns=['ocean_proximity'])\n",
        "X = sample1_ohe.drop('median_house_value', axis=1)\n",
        "y = sample1_ohe['median_house_value']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.13, random_state=12)\n",
        "dt = DecisionTreeRegressor(max_depth=10, random_state=12)\n",
        "dt.fit(X_train, y_train)\n",
        "dt.score(X_train, y_train)\n",
        "dt.score(X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample2 = data1.sample(500, random_state=56)\n",
        "sample2.isna().sum()\n",
        "sample2['total_bedrooms'] = sample2['total_bedrooms'].fillna(sample2['total_bedrooms']).mean()\n",
        "sample2.isna().sum()\n",
        "sample2.head()\n",
        "sample2_ohe = pd.get_dummies(sample2, columns=['ocean_proximity'])\n",
        "X = sample2_ohe.drop('median_house_value', axis=1)\n",
        "y = sample2_ohe['median_house_value']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.13, random_state=12)\n",
        "dtr2 = DecisionTreeRegressor(max_depth=10, random_state=12)\n",
        "dtr2.fit(X_train, y_train)\n",
        "print(f\"Train accuracy: {dtr2.score(X_train, y_train): .4f}\")\n",
        "print(f\"Test accuracy: {dtr2.score(X_test, y_test): .4f}\")\n",
        "dtr2.feature_importances_\n",
        "dtr2.feature_names_in_\n",
        "pd.Series(data = dtr2.feature_importances_, index=dtr2.feature_names_in_).sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeNXwVzGuuLu"
      },
      "source": [
        "<!-- - Visualize the data using a scatter plot (x, y) using code below\n",
        "\n",
        "\n",
        "- Apply DBSCAN clustering and identify core\n",
        "- Count how many points fall into noise vs clusters. -->\n",
        "\n",
        "X = data2[['x','y']]\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
        "clusters = dbscan.fit_predict(X)\n",
        "data2\n",
        "data2['cluster'] = clusters\n",
        "data2\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "unique_labels = np.unique(data2['cluster'])\n",
        "colors = {-1: 'red', 0: 'blue', 1: 'green', 2: 'purple'}  # Extend as needed\n",
        "\n",
        "for label in unique_labels:\n",
        "    mask = data2['cluster'] == label\n",
        "    cluster_name = \"Noise\" if label == -1 else f\"Cluster {label}\"\n",
        "    plt.scatter(data2[mask]['x'], data2[mask]['y'],\n",
        "                c=colors.get(label, 'gray'), s=10, label=cluster_name)\n",
        "plt.title(\"DBSCAN Clustering with Core and Border Points\")\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "n_clusters = len(set(data2['cluster'])) - (1 if -1 in data2['cluster'].values else 0)\n",
        "n_noise = np.sum(data2['cluster'] == -1)\n",
        "\n",
        "print(f\"Number of clusters: {n_clusters}\")\n",
        "print(f\"Number of noise: {n_noise}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implement Agglomerative Clustering for data 2 given in task 4\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "agg = AgglomerativeClustering().fit(X)\n",
        "ac_predictions=agg.fit_predict(X)\n",
        "ac_predictions\n",
        "agg.labels_\n",
        "\n",
        "#option1:use dendogram (ask if we need to use dendogram)\n",
        "import scipy.cluster.hierarchy as sch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "Z = sch.linkage(X, method='ward')  # You can try other linkage methods like 'average', 'single', etc.\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "sch.dendrogram(Z)\n",
        "plt.title(\"Dendrogram\")\n",
        "plt.xlabel(\"Samples\")\n",
        "plt.ylabel(\"Distance\")\n",
        "plt.show()\n",
        "\n",
        "# The silhouette score measures how well each sample fits within its cluster. A higher silhouette score indicates that the clusters are well-defined.\n",
        "\n",
        "# Silhouette score ranges from -1 to +1.\n",
        "\n",
        "# - A score close to +1 indicates that the samples are well clustered.\n",
        "# - A score close to 0 indicates overlapping clusters.\n",
        "# - A negative score suggests that samples might have been assigned to the wrong clusters.\n",
        "\n",
        "#Option 2: Use Silhouette Score\n",
        "#Use silhouette score to find the best number of clusters:\n",
        "from sklearn.metrics import silhouette_score\n",
        "sil_score = silhouette_score(X, ac_predictions)\n",
        "print(f\"Silhouette Score: {sil_score}\")\n",
        "\n",
        "\n",
        "# The silhouette score measures how well each sample fits within its cluster. A higher silhouette score indicates that the clusters are well-defined.\n",
        "\n",
        "# Silhouette score ranges from -1 to +1.\n",
        "\n",
        "# - A score close to +1 indicates that the samples are well clustered.\n",
        "# - A score close to 0 indicates overlapping clusters.\n",
        "# - A negative score suggests that samples might have been assigned to the wrong clusters.\n",
        "\n",
        "\n",
        "\n",
        "# Silhouette Score: 0.31444833716290493\n",
        "# The above silhouette score indicates that the clusters are not well-defined. And may overlap each other. Therefore, the cluster doesn't capture the underlying structure in the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implement K-means clustering for data used in task\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)  # Set a random_state for reproducibility\n",
        "clusters2 = kmeans.fit(data2)  # Fit the KMeans model to the data\n",
        "\n",
        "clusters2.predict(data2)\n",
        "kmeans.cluster_centers_.shape\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Select only 'x' and 'y' for clustering\n",
        "X = data2[['x', 'y']].values\n",
        "\n",
        "link = linkage(X[:15,:], method='ward')  # X - data used\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "dendrogram(link,\n",
        "           orientation='top',\n",
        "           distance_sort='ascending',\n",
        "           show_leaf_counts=False,\n",
        "           truncate_mode='level')\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.show()\n",
        "\n",
        "#extra last\n",
        "# - Analyze: Why is DBSCAN more effective for this kind of spatial distribution compared to K-means?\n",
        "# 1. DBSCAN clusters by density, not distance to a centroid:  \n",
        "#     - DBSCAN grows clusters by \"chaining\" together points that lie in high density\n",
        "# 2. No assumption of COnvex or Spherical shape: \n",
        "#     - KMeans optimizes around a single center and creates convex (ball-shaped) clusters. \n",
        "#     - DBSCAN links nearby points, regardless of overall shape, so it follws the curving geometry of each ring\n",
        "# 3. Automaticaly detects number of clusters: \n",
        "#     - DBSCAN discovers number of clusters based on density thresholds.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Based on the hierarchical clustering dendrogram generated using below code, if you decide to form 3 distinct clusters, identify and list the data points assigned to each of the resulting clusters\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Select only 'x' and 'y' for clustering\n",
        "X = data2[['x', 'y']].values\n",
        "\n",
        "link = linkage(X[:15,:], method='ward')  # X - data used\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "dendrogram(link,\n",
        "           orientation='top',\n",
        "           distance_sort='ascending',\n",
        "           show_leaf_counts=False,\n",
        "           truncate_mode='level')\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.show()\n",
        "\n",
        "# If we cut the tree to form 3 clusters, the points in each are:\n",
        "# - Cluster 1 (orange): 1, 3, 10\n",
        "# - Cluster 2 (green): 13, 8, 4, 12, 14\n",
        "# - Cluster 3 (red): 6, 0, 11, 7, 9, 2, 5\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rW2bmgP-alE"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#oneht\n",
        "# Select the features and target variable\n",
        "features = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked']\n",
        "target = 'Survived'\n",
        "X = titanic_data[features]\n",
        "y = titanic_data[target]\n",
        "\n",
        "numeric_features = ['Age', 'Fare']\n",
        "categorical_features = ['Pclass', 'Sex', 'Embarked']\n",
        "\n",
        "\n",
        "X[categorical_features]\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', SimpleImputer(strategy='mean'), numeric_features),\n",
        "        ('cat', OneHotEncoder(), categorical_features)\n",
        "    ])\n",
        "\n",
        "X = preprocessor.fit_transform(X)\n",
        "X\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
